{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CompVis Stable Diffusion txt2img Colab notebook\n",
    "\n",
    "### A fast and high fidelity diffusion model for the masses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1.\n",
    "\n",
    "### Download your model checkpoint from the internet and upload it to your Google drive (Must have atleast 3 gigabyte of space available) . Make sure to verify the SHA-256 hash using 'sha256sum $checkpoint.ckpt' to ensure the model was not corrupted.\n",
    "\n",
    "### Here is a link on Reddit on how to download the checkpoints. [Reddit link](https://www.reddit.com/r/StableDiffusion/comments/wv4sqt/checkpoint_v14_mirror_no_huggingface_account/)\n",
    "\n",
    "### Select the GPU instance if you haven't already to get ready.\n",
    "\n",
    "\n",
    "##### *As always, I am simply standing on the shoulder of giants here. A massive thank you CompVis, LAION and stability.ai for making this possible.*\n",
    "\n",
    "#### This version is a less confusing way of using the model compared to using the Huggingface diffusers method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi -L #@markdown # Make sure this says T4, P100, V100 or A100. Do not use this with a P4 or K80 as they will not work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2.\n",
    "\n",
    "Mount your Google drive and authenticate your account with Colab. You only need to this once per session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3.\n",
    "\n",
    "Run this cell to clone the code and install dependency packages. You only need to this once per session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers==0.2.4\n",
    "!pip install transformers scipy ftfy\n",
    "!pip install dotmap\n",
    "!git clone -b colab https://github.com/JohnnyRacer/stable-diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_drive = True\n",
    "model_path = \"/content/drive/MyDrive/AI/SD/weights\" # Change this to your path\n",
    "outputs_path =   \"/content/drive/MyDrive/AI/SD/outputs\" if save_to_drive else \"./outputs\"\n",
    "skip_installs = True\n",
    "latent_diffusion_model = 'sd-v1-4' #\n",
    "model_fp = os.path.join(model_path, f'{latent_diffusion_model}.ckpt' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4.\n",
    "\n",
    "Run this cell to import the required packages and load the model. You only need to this once per session.\n",
    "\n",
    "##### *If the model load fails due to missing dict keys, then your model is most likely corrupt. Try deleting from your drive and reuploading to try again.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse, os, sys, glob\n",
    "sys.path.append('./stable-diffusion')\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from omegaconf import OmegaConf\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from itertools import islice\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "import time\n",
    "from pytorch_lightning import seed_everything\n",
    "from torch import autocast\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms as T\n",
    "from torchvision.transforms import functional as TF\n",
    "from contextlib import contextmanager, nullcontext\n",
    "from ldm.util import instantiate_from_config\n",
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "from ldm.models.diffusion.plms import PLMSSampler\n",
    "from diffusers.pipelines.stable_diffusion.safety_checker import StableDiffusionSafetyChecker\n",
    "from transformers import AutoFeatureExtractor\n",
    "import io\n",
    "\n",
    "def load_model_from_config(config, ckpt, verbose=False,use_half=False):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    if len(m) > 0 and verbose:\n",
    "        print(\"missing keys:\")\n",
    "        print(m)\n",
    "    if len(u) > 0 and verbose:\n",
    "        print(\"unexpected keys:\")\n",
    "        print(u)\n",
    "\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    if use_half:\n",
    "        model.half()\n",
    "    return model\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "config = OmegaConf.load(\"stable-diffusion/configs/stable-diffusion/v1-inference.yaml\")  # TODO: Optionally download from same location as ckpt and chnage this logic\n",
    "model = load_model_from_config(config,model_fp)  # TODO: check path\n",
    "if torch.cuda.is_available(): # Use fp16 whenever on GPU.  Running in full precision just slows everything down. Especially on Volta and newer cards (Turing, Ampere) .\n",
    "    model = model.to(device).eval().half() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "    \n",
    "    w, h = imgs[0].size\n",
    "    grid = Image.new('RGB', size=(cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    \n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid\n",
    "\n",
    "\n",
    "def check_safety(x_image):\n",
    "    safety_checker_input = safety_feature_extractor(numpy_to_pil(x_image), return_tensors=\"pt\")\n",
    "    x_checked_image, has_nsfw_concept = safety_checker(images=x_image, clip_input=safety_checker_input.pixel_values)\n",
    "    print(x_checked_image)\n",
    "    print(x_image)\n",
    "    print(type(x_checked_image))\n",
    "    print(type(x_image))\n",
    "    assert x_checked_image.shape[0] == len(has_nsfw_concept)\n",
    "    for i in range(len(has_nsfw_concept)):\n",
    "        if has_nsfw_concept[i]:\n",
    "            x_checked_image[i] = load_replacement(x_checked_image[i])\n",
    "    return x_image, False\n",
    "\n",
    "def dummy_safety_check(x_image):\n",
    "    return x_image, False\n",
    "\n",
    "\n",
    "def numpy_to_pil(images):\n",
    "    \"\"\"\n",
    "    Convert a numpy image or a batch of images to a PIL image.\n",
    "    \"\"\"\n",
    "    if images.ndim == 3:\n",
    "        images = images[None, ...]\n",
    "    images = (images * 255).round().astype(\"uint8\")\n",
    "    pil_images = [Image.fromarray(image) for image in images]\n",
    "\n",
    "    return pil_images\n",
    "\n",
    "def load_replacement(x):\n",
    "    try:\n",
    "        hwc = x.shape\n",
    "        y = Image.open(\"assets/rick.jpeg\").convert(\"RGB\").resize((hwc[1], hwc[0]))\n",
    "        y = (np.array(y)/255.0).astype(x.dtype)\n",
    "        assert y.shape == x.shape\n",
    "        return y\n",
    "    except Exception:\n",
    "        return x\n",
    "\n",
    "\n",
    "def display_handler(x,i,cadance = 5, decode = True,save_image=True):\n",
    "    img_tensor = x\n",
    "    if i%cadance==0:\n",
    "        if decode: \n",
    "            x = model.decode_first_stage(x)\n",
    "        grid = make_grid(torch.clamp((x+1.0)/2.0, min=0.0, max=1.0),round(x.shape[0]**0.5+0.2))\n",
    "        grid = 255. * rearrange(grid, 'c h w -> h w c').detach().cpu().numpy()\n",
    "        image_grid = grid.copy(order = \"C\") \n",
    "        with io.BytesIO() as output:\n",
    "            im = Image.fromarray(grid.astype(np.uint8))\n",
    "            #display.display(im)\n",
    "            if save_image:\n",
    "                im.save(output, format = \"PNG\")\n",
    "\n",
    "\n",
    "\n",
    "from torch import autocast\n",
    "def do_run():\n",
    "    opt.output_images = []\n",
    "    if opt.plms:\n",
    "        sampler = PLMSSampler(model)\n",
    "    else:\n",
    "        sampler = DDIMSampler(model)\n",
    "    outpath = opt.outdir\n",
    "    batch_size = opt.n_samples\n",
    "    data = [batch_size * [opt.prompt]]\n",
    "    #os.makedirs(opt.outdir, exist_ok=True)\n",
    "  \n",
    "    n_rows = opt.n_rows if opt.n_rows > 0 else batch_size\n",
    "    if opt.fixed_code:\n",
    "        start_code = torch.randn([opt.n_samples, opt.C, opt.H // opt.f, opt.W // opt.f], device=device)\n",
    "    precision_scope = autocast if opt.precision==\"autocast\" else nullcontext\n",
    "    with torch.no_grad():\n",
    "        with precision_scope(\"cuda\"):\n",
    "            with model.ema_scope():\n",
    "                all_samples = list()\n",
    "                for n in trange(opt.n_iter, desc=\"Sampling\"):\n",
    "                    for prompts in tqdm(data, desc=\"data\"):\n",
    "                        uc = None\n",
    "                        if opt.scale != 1.0:\n",
    "                            uc = model.get_learned_conditioning(batch_size * [\"\"])\n",
    "                        c = model.get_learned_conditioning([opt.prompt])\n",
    "                        shape = [opt.C, opt.H // opt.f, opt.W // opt.f]\n",
    "                        samples_ddim, _ = sampler.sample(S=opt.ddim_steps,\n",
    "                                                         conditioning=c,\n",
    "                                                         batch_size=opt.n_samples,\n",
    "                                                         shape=shape,\n",
    "                                                         verbose=False,\n",
    "                                                         unconditional_guidance_scale=opt.scale,\n",
    "                                                         unconditional_conditioning=uc,\n",
    "                                                         eta=opt.ddim_eta,\n",
    "                                                         img_callback=display_handler\n",
    "                                                         x_T=start_code)\n",
    "\n",
    "                        x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "                        x_samples_ddim = torch.clamp((x_samples_ddim + 1.0) / 2.0, min=0.0, max=1.0)\n",
    "                        x_samples_ddim = x_samples_ddim.cpu().permute(0, 2, 3, 1).numpy()\n",
    "                        \n",
    "                        safety_func = check_safety if opt.use_safety else dummy_safety_check\n",
    "                        \n",
    "                        x_checked_image, has_nsfw_concept = safety_func(x_samples_ddim)\n",
    "\n",
    "                        x_checked_image_torch = torch.from_numpy(x_checked_image).permute(0, 3, 1, 2)\n",
    "\n",
    "                        if not opt.skip_save:\n",
    "                            for x_sample in x_checked_image_torch:\n",
    "                                x_sample = 255. * rearrange(x_sample.cpu().numpy(), 'c h w -> h w c')\n",
    "                                img = Image.fromarray(x_sample.astype(np.uint8))\n",
    "                                opt.output_images.append(img)\n",
    "                                #img = put_watermark(img, wm_encoder)\n",
    "                                #img.save(os.path.join(sample_path, f\"{base_count:05}.png\"))\n",
    "                                #base_count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotmap import DotMap\n",
    "\n",
    "opt = DotMap()\n",
    "opt.precision= \"autocast\" if torch.cuda.is_available() else \"cpu\"\n",
    "opt.prompt = \"A watercolor portrait of a geisha princess with tattoos with intricate floral patterns background, trending on artstation\"\n",
    "opt.uc = \"\" # Optional negative prompt\n",
    "opt.plms = False \n",
    "opt.scale = 8.5\n",
    "opt.W = 512\n",
    "opt.H = 512\n",
    "opt.n_iter = 1\n",
    "opt. ddim_eta = 0.0\n",
    "opt.ddim_steps = 50\n",
    "opt.n_samples = 1 # Samples you want\n",
    "opt.n_rows = 0\n",
    "opt.f = 8\n",
    "opt.C = 4\n",
    "opt.fixed_code = True\n",
    "opt.use_safety = False # Set this to true for the NSFW version.\n",
    "opt.make_grid = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt.use_safety:\n",
    "    safety_model_id = \"CompVis/stable-diffusion-safety-checker\"\n",
    "    safety_feature_extractor = AutoFeatureExtractor.from_pretrained(safety_model_id)\n",
    "    safety_checker = StableDiffusionSafetyChecker.from_pretrained(safety_model_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_run()\n",
    "\n",
    "if opt.make_grid:\n",
    "    rows = 3\n",
    "    columns = 3\n",
    "    grid = image_grid(opt.output_images, rows=rows, cols=columns)\n",
    "    display(grid)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
